#!/usr/bin/env python3
"""
poundwholesale-co-uk Product Extraction Script
Auto-generated by supplier_script_generator.py on 2025-07-05T20:51:19.477536

This script provides product data extraction for https://www.poundwholesale.co.uk/
Integrates with LangGraph workflow and configurable supplier scraper.
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

# Configuration
SUPPLIER_URL = "https://www.poundwholesale.co.uk/"
SUPPLIER_ID = "poundwholesale-co-uk"
CACHE_DIR = Path("../cache")

class Poundwholesale_Co_UkProductExtractor:
    """Product extraction for poundwholesale-co-uk"""
    
    def __init__(self):
        self.page = None
        self.cache_file = CACHE_DIR / f"{SUPPLIER_ID}_products.json"
        self.config_file = Path("../config/product_selectors.json")
        
        # Ensure cache directory exists
        CACHE_DIR.mkdir(exist_ok=True)
    
    async def extract_products(self, page, start_page=1, max_pages=None):
        """Extract products from supplier pages"""
        try:
            log.info(f"ðŸ•·ï¸ Starting product extraction from {SUPPLIER_URL}")
            
            self.page = page
            products = []
            current_page = start_page
            
            while True:
                log.info(f"ðŸ“„ Processing page {current_page}")
                
                # Navigate to page
                page_url = f"{SUPPLIER_URL}?page={current_page}"
                await self.page.goto(page_url, wait_until='domcontentloaded')
                await self.page.wait_for_load_state('networkidle', timeout=10000)
                
                # Extract products from current page
                page_products = await self.extract_page_products()
                
                if not page_products:
                    log.info("No more products found - stopping extraction")
                    break
                
                products.extend(page_products)
                log.info(f"âœ… Extracted {len(page_products)} products from page {current_page}")
                
                current_page += 1
                
                # Check max pages limit
                if max_pages and current_page > start_page + max_pages - 1:
                    log.info(f"Reached max pages limit: {max_pages}")
                    break
                
                # Save progress periodically
                if current_page % 5 == 0:
                    await self.save_products(products)
            
            # Final save
            await self.save_products(products)
            
            log.info(f"ðŸŽ‰ Extraction complete! Total products: {len(products)}")
            return products
            
        except Exception as e:
            log.error(f"Product extraction failed: {e}")
            return []
    
    async def extract_page_products(self):
        """Extract products from current page"""
        try:
            # Load product selectors configuration
            if self.config_file.exists():
                with open(self.config_file, 'r') as f:
                    selectors = json.load(f)
            else:
                # Default selectors - will be auto-discovered
                selectors = {
                    "product_item": [".product-item", ".product", "article"],
                    "title": [".product-title", ".title", "h2", "h3"],
                    "price": [".price", ".cost", "[data-price]"],
                    "url": ["a", ".product-link"],
                    "image": ["img", ".product-image"]
                }
            
            products = []
            
            # Find product containers
            product_containers = await self.page.query_selector_all(selectors["product_item"][0])
            
            for container in product_containers:
                try:
                    product = await self.extract_single_product(container, selectors)
                    if product:
                        products.append(product)
                except Exception as e:
                    log.warning(f"Failed to extract product: {e}")
                    continue
            
            return products
            
        except Exception as e:
            log.error(f"Page extraction failed: {e}")
            return []
    
    async def extract_single_product(self, container, selectors):
        """Extract data from single product container"""
        try:
            product = {
                "extracted_at": datetime.now().isoformat(),
                "supplier_id": SUPPLIER_ID,
                "supplier_url": SUPPLIER_URL
            }
            
            # Extract title
            for selector in selectors.get("title", []):
                try:
                    title_element = await container.query_selector(selector)
                    if title_element:
                        product["title"] = await title_element.inner_text()
                        break
                except:
                    continue
            
            # Extract price
            for selector in selectors.get("price", []):
                try:
                    price_element = await container.query_selector(selector)
                    if price_element:
                        price_text = await price_element.inner_text()
                        product["price"] = price_text.strip()
                        break
                except:
                    continue
            
            # Extract URL
            for selector in selectors.get("url", []):
                try:
                    url_element = await container.query_selector(selector)
                    if url_element:
                        href = await url_element.get_attribute('href')
                        if href:
                            product["url"] = href if href.startswith('http') else f"{SUPPLIER_URL.rstrip('/')}{href}"
                            break
                except:
                    continue
            
            # Extract image
            for selector in selectors.get("image", []):
                try:
                    img_element = await container.query_selector(selector)
                    if img_element:
                        src = await img_element.get_attribute('src')
                        if src:
                            product["image"] = src if src.startswith('http') else f"{SUPPLIER_URL.rstrip('/')}{src}"
                            break
                except:
                    continue
            
            # Only return if we have minimum required fields
            if product.get("title") and (product.get("price") or product.get("url")):
                return product
            
            return None
            
        except Exception as e:
            log.error(f"Single product extraction failed: {e}")
            return None
    
    async def save_products(self, products):
        """Save products to cache file"""
        try:
            cache_data = {
                "supplier_id": SUPPLIER_ID,
                "supplier_url": SUPPLIER_URL,
                "extracted_at": datetime.now().isoformat(),
                "total_products": len(products),
                "products": products
            }
            
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2)
            
            log.info(f"ðŸ’¾ Saved {len(products)} products to cache")
            
        except Exception as e:
            log.error(f"Failed to save products: {e}")

# Standalone function for integration
async def extract_poundwholesale_co_uk_products(page, start_page=1, max_pages=None):
    """
    Standalone product extraction function
    
    Args:
        page: Playwright page object
        start_page: Starting page number
        max_pages: Maximum pages to process
    
    Returns:
        list: Extracted products
    """
    try:
        extractor = Poundwholesale_Co_UkProductExtractor()
        return await extractor.extract_products(page, start_page, max_pages)
    except Exception as e:
        log.error(f"Standalone extraction failed: {e}")
        return []

if __name__ == "__main__":
    # Test the extraction functionality
    async def test_extraction():
        from playwright.async_api import async_playwright
        
        playwright = await async_playwright().start()
        browser = await playwright.chromium.connect_over_cdp("http://localhost:9222")
        context = browser.contexts[0] if browser.contexts else await browser.new_context()
        page = context.pages[0] if context.pages else await context.new_page()
        
        extractor = Poundwholesale_Co_UkProductExtractor()
        products = await extractor.extract_products(page, max_pages=2)
        
        log.info(f"âœ… Test extraction completed: {len(products)} products")
    
    asyncio.run(test_extraction())
